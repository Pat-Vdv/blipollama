# BLIPollama â€“ Image Captioning (BLIP & Ollama)

BLIPollama est un outil Python permettant de gÃ©nÃ©rer automatiquement des **descriptions dâ€™images (captions)** Ã  partir dâ€™un rÃ©pertoire local, en utilisant soit :

- **BLIP** (via Hugging Face Transformers, exÃ©cution locale CPU/GPU)
- **Ollama** (via un modÃ¨le multimodal local comme `llava`)

Lâ€™architecture est volontairement **propre et modulaire** :
- le service est agnostique des modÃ¨les
- les backends (BLIP / Ollama) sont injectÃ©s
- une CLI permet de choisir facilement le moteur

---

## âœ¨ FonctionnalitÃ©s

- ğŸ“ Analyse dâ€™un rÃ©pertoire dâ€™images
- ğŸ–¼ï¸ Filtres par extensions (`jpg`, `png`, â€¦)
- ğŸ” Mode rÃ©cursif optionnel
- ğŸ§  Choix du backend : BLIP ou Ollama
- ğŸ“Š Barre de progression (`tqdm`)
- ğŸ§ª Tests unitaires (`pytest`)
- ğŸ“¦ Structure prÃªte pour un vrai package Python

---

## ğŸ§± PrÃ©requis

- Python **3.10+**
- (Optionnel) GPU CUDA si utilisation de BLIP avec accÃ©lÃ©ration
- (Optionnel) Ollama installÃ© et lancÃ© localement

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Cloner le projet
```bash
git clone <ton-repo-git>
cd IBM
2ï¸âƒ£ CrÃ©er et activer un environnement virtuel
python -m venv venv
source venv/bin/activate
3ï¸âƒ£ Installer les dÃ©pendances
pip install -r requirements.txt
âš ï¸ Remarque :
Le package torch peut nÃ©cessiter une installation spÃ©cifique selon ton GPU / CUDA.
RÃ©f. : https://pytorch.org/get-started/locally/

ğŸ§ª VÃ©rifier lâ€™installation
Lancer les tests unitaires
pytest
RÃ©sultat attendu :

1 passed in X.XXs
ğŸš€ Utilisation (CLI)
Exemple avec BLIP
python main.py \
  --backend blip \
  --dir /mnt/d/Photos/100MEDIA \
  --ext jpg jpeg png \
  --out captions_blip.txt
Exemple avec Ollama
python main.py \
  --backend ollama \
  --dir /mnt/d/Photos/100MEDIA \
  --ext jpg png \
  --ollama-model llava \
  --out captions_ollama.txt
Options principales
Option	Description
--backend	blip ou ollama
--dir	RÃ©pertoire dâ€™images Ã  analyser
--ext	Extensions Ã  inclure
--recursive	Analyse rÃ©cursive
--out	Fichier de sortie
ğŸ§  Architecture du projet
IBM/
â”œâ”€ blipollama/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ models.py        # Protocol + dataclasses
â”‚  â””â”€ service.py       # Orchestrateur (agnostique)
â”œâ”€ tests/
â”‚  â””â”€ test_service_minimal.py
â”œâ”€ main.py             # CLI & configuration
â”œâ”€ requirements.txt
â”œâ”€ pytest.ini
â””â”€ README.md
Principe clÃ©
Le service ne connaÃ®t pas les modÃ¨les
Il reÃ§oit un objet capable de produire une caption (caption(Path) -> str)

ğŸ§© Ajouter un nouveau backend
Pour ajouter un moteur (ex: Florence, API externe, etc.) :

class MyBackend:
    name = "mybackend"

    def caption(self, image_path: Path) -> str:
        return "my caption"
Puis lâ€™injecter dans VisionCaptionService.

Aucune modification du service nâ€™est nÃ©cessaire.

ğŸ§¹ Git & bonnes pratiques
Certains fichiers sont volontairement ignorÃ©s :

environnements virtuels

caches Python / pytest

fichiers gÃ©nÃ©rÃ©s (captions_*.txt)

images locales

Voir .gitignore.

ğŸ“Œ Notes
BLIP offre des captions plus structurÃ©es et prÃ©cises

Ollama est plus simple Ã  dÃ©ployer si tu as dÃ©jÃ  un stack local LLM

Le projet est prÃªt pour :

un fallback automatique BLIP â†’ Ollama

une intÃ©gration RAG

une base de donnÃ©es (SQL / vectorielle)

ğŸ“„ Licence
Projet pÃ©dagogique / expÃ©rimental.
Ã€ adapter selon ton usage (personnel / pro).

ğŸ™Œ Auteur
Patrick Vandervoort
(Projet Coursera / IBM â€“ Vision & LLM)
